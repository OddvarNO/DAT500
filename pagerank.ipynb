{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from operator import add\n",
    "from pyspark.sql.functions import explode_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as f\n",
    "# # Read data\n",
    "# df = spark.read.json('/home/ubuntu/processed_data.json')\n",
    "\n",
    "# # Convert the profane words to a DataFrame\n",
    "# PROFANE_WORDS = ['omorashi', 'blowjob', 'scat', 'snowballing', 'shitty', 'tits', 'tribadism', 'strap on', 'hot carl', 'wank', 'twat', 'zoophilia', 'spic', 'homoerotic', 'pisspig', 'jailbait', 'bastardo', 'shemale', 'xx', 'butthole', 'yaoi', 'kinkster', 'bastard', 'faggot', 'shit', 'frotting', 'beaner', 'baby juice', 'poontang', 'sultry women', 'female squirting', 'nigga', 'strapon', 's&m', 'gay sex', 'nudity', 'darkie', 'vibrator', 'genitals', 'male squirting', 'birdlock', 'date rape', 'double penetration', 'sucks', 'tub girl', 'rapist', 'urophilia', 'fisting', 'blumpkin', 'piece of shit', 'poop chute', 'nigger', 'poon', 'doggiestyle', 'rimjob', 'splooge', 'bullshit', 'ponyplay', 'yiffy', 'dvda', 'alabama hot pocket', 'beaver lips', 'double dong', 'donkey punch', 'slanteye', 'eat my ass', 'babeland', 'wetback', 'spread legs', 'bondage', 'nambla', 'feltch', 'masturbate', 'juggs', 'kinbaku', 'twinkie', 'bollocks', 'fucking', 'pedobear', 'beaners', 'dendrophilia', 'erotism', 'pussy', 'barenaked', 'queaf', 'porn', 'twink', 'knobbing', 'neonazi', 'raping', 'shitblimp', 'coon', 'blonde action', 'reverse cowgirl', 'spooge', 'girl on', 'acrotomophilia', 'porno', 'creampie', 'piss pig', 'doggie style', 'futanari', 'femdom', 'hentai', 'towelhead', 'god damn', 'topless', 'bangbros', 'sodomy', 'nig nog', 'bulldyke', 'black cock', 'clusterfuck', 'bukkake', 'felch', 'jigaboo', 'white power', 'boob', 'guro', 'rectum', 'doggy style', 'kinky', 'sexy', 'coprolagnia', 'undressing', 'muffdiving', 'blow job', 'dolcett', 'blonde on blonde action', 'shibari', 'punany', 'ass', 'tushy', 'anal', 'hooker', 'humping', 'dildo', 'wrinkled starfish', 'ball gag', 'gang bang', 'g-spot', 'pleasure chest', 'dry hump', 'camgirl', 'brown showers', 'anilingus', 'sodomize', 'bestiality', 'coons', 'carpetmuncher', 'schlong', 'auto erotic', 'lemon party', 'goatcx', 'intercourse', 'hand job', '2g1c', 'nude', 'domination', 'boner', 'boobs', 'make me come', 'kike', 'group sex', 'bimbos', 'how to kill', 'jail bait', 'fellatio', 'circlejerk', 'menage a trois', 'style doggy', 'queef', 'goodpoop', 'pissing', 'alaskan pipeline', 'panty', 'girls gone wild', 'tainted love', 'rosy palm and her 5 sisters', 'muff diver', 'autoerotic', 'goregasm', 'jack off', 'orgasm', 'dingleberry', 'bung hole', 'handjob', 'dirty sanchez', 'quim', 'mound of venus', 'pole smoker', 'rosy palm', 'big tits', 'big knockers', 'shrimping', 'threesome', 'poopchute', 'grope', 'strip club', 'two girls one cup', 'honkey', 'motherfucker', 'tongue in a', 'strappado', 'asshole', 'cornhole', 'goatse', 'jerk off', 'blue waffle', 'violet wand', 'deep throat', 'butt', 'dog style', 'camslut', 'dick', 'deepthroat', 'barely legal', 'fucktards', 'fuckin', 'dp action', 'arsehole', 'wrapping men', 'dominatrix', 'santorum', 'swastika', 'tight white', 'big breasts', 'jiggerboo', 'anus', 'upskirt', 'cunnilingus', 'titty', 'buttcheeks', 'doggystyle', 'bareback', 'playboy', 'nipple', 'camwhore', 'mr hands', 'big black', 'fingering', 'how to murder', 'brunette action', 'milf', 'cock', 'sadism', 'foot fetish', 'giant cock', 'phone sex', 'huge fat', 'penis', 'vagina', 'ejaculation', 'voyeur', 'shaved beaver', 'shota', 'pedophile', 'camel toe', 'jiggaboo', 'dommes', 'carpet muncher', 'busty', 'rimming', 'paedophile', 'suicide girls', 'orgy', 'nsfw images', 'escort', 'booty call', 'rape', 'bastinado', 'cunt', 'nipples', 'tied up', 'scissoring', 'titties', 'taste my', 'sex', 'spunk', 'cum', 'semen', 'baby batter', 'prince albert piercing', 'one cup two girls', 'urethra play', 'ecchi', 'fingerbang', 'missionary position', 'tubgirl', 'vorarephilia', 'jizz', 'nimphomania', 'wet dream', 'xxx', 'nympho', 'ball kicking', 'erotic', 'tit', 'golden shower', 'goo girl', 'blow your load', 'one guy one jar', 'ball sucking', 'assmunch', 'incest', 'pornography', 'coprophilia', 'lolita', 'poof', 'nymphomania', 'eunuch', 'cocks', 'fudge packer', 'pegging', 'pthc', 'hardcore', 'paki', 'jelly donut', 'tosser', 'panties', 'shaved pussy', 'bdsm', 'sexo', 'gokkun', 'venus mound', 'chocolate rosebuds', 'footjob', 'pubes', 'bitches', 'beaver cleaver', 'clitoris', '2 girls 1 cup', 'hard core', 'swinger', 'skeet', 'clover clamps', 'dirty pillows', 'ball licking', 'raging boner', 'ball sack', 'leather restraint', 'hot chick', 'throating', 'rusty trombone', 'splooge moose', 'bullet vibe', 'bunghole', 'fecal', 'suck', 'leather straight jacket', 'octopussy', 'vulva', 'tranny', 'slut', 'fuck buttons', 'snatch', 'girl on top', 'negro', 'dingleberries', 'bbw', 'cumming', 'tea bagging', 'ball gravy', 'nawashi', 'cleveland steamer', 'daterape', 'apeshit', 'lovemaking', 'raghead', 'fudgepacker', 'yellow showers', 'clit', 'smut', 'fuck', 'figging', 'bitch']\n",
    "# df_pw = spark.createDataFrame([Row(arr=PROFANE_WORDS)])\n",
    "# df_pw.groupBy().pivot('arr').agg(f.first('Value')).show()\n",
    "\n",
    "# # Tokenize the text\n",
    "# tokenizer = Tokenizer(inputCol='parsed_text', outputCol=\"tokens\")\n",
    "# tokenizer.transform(df).select('tokens')\n",
    "\n",
    "\n",
    "# # OLD METHOD\n",
    "# lines_testfile = spark.read.text('testfile.txt').rdd.map(lambda r: r[0])\n",
    "# print(lines_testfile.take(5))\n",
    "\n",
    "\n",
    "# # NEW METHOD\n",
    "# #rdd_data = spark.read.json('processed_data.json')\n",
    "# exploded = rdd_data.select('url', explode_outer(rdd_data.neighbor_urls).alias(\"neighbour_url\"))\n",
    "# lines_json = exploded.rdd.map(lambda x: x)\n",
    "# print(lines_json.take(5))\n",
    "\n",
    "\n",
    "# links = lines_json.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "# #print(links.take(5))\n",
    "# #links.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/27 13:29:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://gmpg.org has rank: 0.20930496183490793.\n",
      "http://fly.carlosparaglide.com has rank: 0.20930496183490793.\n",
      "https://api.w.org has rank: 0.20930496183490793.\n"
     ]
    }
   ],
   "source": [
    "# Code copied from: https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    return urls[0], urls[1]\n",
    "\n",
    "\n",
    "def pageRank(file, iters = 10):\n",
    "    \"\"\"Naive PageRank algorithm.\"\"\"\n",
    "    \n",
    "    # Initialize the spark context.\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPageRank\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Loads in input file. It should be in format of:\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     ...\n",
    "\n",
    "\n",
    "    # Read the data\n",
    "    df = spark.read.json(file)\n",
    "\n",
    "    # Explode to get the right format.\n",
    "    exploded_df = df.select('url', explode_outer(df.neighbor_urls).alias(\"neighbour_url\"))\n",
    "    lines = exploded_df.rdd.map(lambda x: x)\n",
    "\n",
    "    # Loads all URLs from input file and initialize their neighbors.\n",
    "    links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "    ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "\n",
    "    # Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "    for iteration in range(int(iters)):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "        contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(\n",
    "            url_urls_rank[1][0], url_urls_rank[1][1]  # type: ignore[arg-type]\n",
    "        ))\n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "    # Collects all URL ranks and dump them to console.\n",
    "    for (link, rank) in ranks.collect():\n",
    "        print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "pageRank('/home/ubuntu/processed_data.json')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
